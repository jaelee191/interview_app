#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCP Playwrightë¥¼ í™œìš©í•œ ì±„ìš©ê³µê³  ìŠ¤ëƒ…ìƒ· ë¶„ì„
ì‚¬ëŒì¸ ë“± ë³µì‚¬ ë°©ì§€ ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë¦°ìƒ·ìœ¼ë¡œ ë¶„ì„
"""

import asyncio
import json
import base64
import sys
from playwright.async_api import async_playwright
from pathlib import Path
import tempfile
import os

async def capture_job_posting_snapshot(url):
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„ìš©ê³µê³  í˜ì´ì§€ ìŠ¤ëƒ…ìƒ· ìº¡ì²˜
    """
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless ëª¨ë“œ)
        browser = await p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        
        page = await context.new_page()
        
        try:
            print(f"ğŸ“¸ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}", file=sys.stderr)
            
            # í˜ì´ì§€ ë¡œë“œ
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # íŒì—…/ëª¨ë‹¬ ë‹«ê¸° ì‹œë„
            popup_selectors = [
                '[class*="close"]',
                '[class*="modal_close"]',
                '[class*="btn_close"]',
                '.layer_close'
            ]
            
            for selector in popup_selectors:
                try:
                    await page.click(selector, timeout=1000)
                    print(f"âœ… íŒì—… ë‹«ê¸°: {selector}", file=sys.stderr)
                except:
                    pass
            
            # ì±„ìš©ê³µê³  ì»¨í…ì¸  ì˜ì—­ ëŒ€ê¸°
            content_selectors = [
                '.wrap_jv_cont',  # ì‚¬ëŒì¸
                '.content',
                '.job_content',
                '[class*="recruit"]',
                '[class*="posting"]'
            ]
            
            content_found = False
            for selector in content_selectors:
                try:
                    await page.wait_for_selector(selector, timeout=5000)
                    content_found = True
                    print(f"âœ… ì»¨í…ì¸  ë°œê²¬: {selector}", file=sys.stderr)
                    break
                except:
                    continue
            
            if not content_found:
                print("âš ï¸ ì±„ìš©ê³µê³  ì»¨í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", file=sys.stderr)
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì»¨í…ì¸  ë¡œë“œ
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(2000)
            await page.evaluate("window.scrollTo(0, 0)")
            
            # ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ (ì „ì²´ í˜ì´ì§€)
            screenshot_bytes = await page.screenshot(
                full_page=True,
                type='png'
            )
            
            # Base64 ì¸ì½”ë”©
            screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (ê°€ëŠ¥í•œ ê²½ìš°)
            text_content = ""
            try:
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_elements = await page.query_selector_all('p, li, h1, h2, h3, h4, span, div')
                texts = []
                for element in text_elements[:500]:  # ë„ˆë¬´ ë§ì€ ìš”ì†Œ ë°©ì§€
                    try:
                        text = await element.inner_text()
                        if text and len(text.strip()) > 5:
                            texts.append(text.strip())
                    except:
                        continue
                
                text_content = "\n".join(texts)
                print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ: {len(text_content)} ê¸€ì", file=sys.stderr)
            except Exception as e:
                print(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = {}
            try:
                metadata['title'] = await page.title()
                metadata['url'] = page.url
                
                # Open Graph íƒœê·¸ ì¶”ì¶œ
                og_tags = await page.evaluate("""
                    () => {
                        const tags = {};
                        const metas = document.querySelectorAll('meta[property^="og:"]');
                        metas.forEach(meta => {
                            const property = meta.getAttribute('property').replace('og:', '');
                            tags[property] = meta.getAttribute('content');
                        });
                        return tags;
                    }
                """)
                metadata['og'] = og_tags
            except:
                pass
            
            result = {
                'success': True,
                'screenshot': screenshot_base64,
                'text': text_content,
                'metadata': metadata,
                'url': url,
                'screenshot_size': len(screenshot_bytes)
            }
            
            print(f"âœ… ìŠ¤ëƒ…ìƒ· ìº¡ì²˜ ì™„ë£Œ: {len(screenshot_bytes)} bytes", file=sys.stderr)
            return result
            
        except Exception as e:
            print(f"âŒ ìŠ¤ëƒ…ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {str(e)}", file=sys.stderr)
            return {
                'success': False,
                'error': str(e),
                'url': url
            }
        
        finally:
            await browser.close()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print(json.dumps({
            'success': False,
            'error': 'URLì´ í•„ìš”í•©ë‹ˆë‹¤'
        }))
        sys.exit(1)
    
    url = sys.argv[1]
    
    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    loop = asyncio.get_event_loop()
    result = loop.run_until_complete(capture_job_posting_snapshot(url))
    
    # JSONìœ¼ë¡œ ì¶œë ¥
    print(json.dumps(result, ensure_ascii=False))

if __name__ == "__main__":
    main()