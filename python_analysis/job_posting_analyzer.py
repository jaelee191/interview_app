#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì±„ìš©ê³µê³  ë¶„ì„ê¸°
- í”Œë ˆì´ë¼ì´íŠ¸ MCPì™€ ì—°ë™í•˜ì—¬ ì±„ìš©ê³µê³  ë°ì´í„° ë¶„ì„
- í‚¤ì›Œë“œ ì¶”ì¶œ, ìš”êµ¬ì‚¬í•­ ë¶„ì„, íŠ¸ë Œë“œ ë¶„ì„
"""

import os
import sys
import json
import re
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any, Optional
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class JobPostingAnalyzer:
    """ì±„ìš©ê³µê³  ë¶„ì„ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.korean_stopwords = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë“¤', 'ì—', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 
            'ìœ¼ë¡œ', 'ë¡œ', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì²˜ëŸ¼',
            'ê°™ì´', 'ë³´ë‹¤', 'ë”', 'ê°€ì¥', 'ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì˜',
            'ì¢‹ì€', 'ë‚˜ìœ', 'í¬ë‹¤', 'ì‘ë‹¤', 'ë§ë‹¤', 'ì ë‹¤', 'ë†’ë‹¤', 'ë‚®ë‹¤',
            'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ'
        }
        
        # ê¸°ìˆ  ìŠ¤íƒ í‚¤ì›Œë“œ
        self.tech_keywords = {
            'languages': ['Python', 'Java', 'JavaScript', 'TypeScript', 'Go', 'Rust', 'C++', 'C#', 'PHP', 'Ruby', 'Swift', 'Kotlin'],
            'frameworks': ['React', 'Vue.js', 'Angular', 'Django', 'Flask', 'Spring', 'Express', 'Next.js', 'Nuxt.js'],
            'databases': ['MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Elasticsearch', 'Oracle', 'SQLite'],
            'cloud': ['AWS', 'GCP', 'Azure', 'Docker', 'Kubernetes', 'Jenkins', 'GitHub Actions'],
            'tools': ['Git', 'Jira', 'Slack', 'Figma', 'Sketch', 'Photoshop', 'Illustrator']
        }
        
        # ì§ë¬´ë³„ í‚¤ì›Œë“œ
        self.job_categories = {
            'developer': ['ê°œë°œì', 'ê°œë°œ', 'Developer', 'Engineer', 'ì—”ì§€ë‹ˆì–´', 'í”„ë¡œê·¸ë˜ë¨¸', 'ì½”ë”©'],
            'designer': ['ë””ìì´ë„ˆ', 'ë””ìì¸', 'Designer', 'UI', 'UX', 'ì‹œê°ë””ìì¸', 'ì›¹ë””ìì¸'],
            'marketing': ['ë§ˆì¼€íŒ…', 'Marketing', 'í™ë³´', 'ë¸Œëœë”©', 'PR', 'ê´‘ê³ ', 'í¼í¬ë¨¼ìŠ¤ë§ˆì¼€íŒ…'],
            'sales': ['ì˜ì—…', 'ì„¸ì¼ì¦ˆ', 'Sales', 'ë¹„ì¦ˆë‹ˆìŠ¤ê°œë°œ', 'BD', 'ê³ ê°ê´€ë¦¬'],
            'data': ['ë°ì´í„°', 'Data', 'ë¶„ì„', 'Analytics', 'ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤', 'ML', 'ë¨¸ì‹ ëŸ¬ë‹']
        }
        
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
        self._download_nltk_data()
    
    def _download_nltk_data(self):
        """NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
            nltk.data.find('taggers/averaged_perceptron_tagger')
            nltk.data.find('chunkers/maxent_ne_chunker')
            nltk.data.find('corpora/words')
        except LookupError:
            print("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            nltk.download('maxent_ne_chunker', quiet=True)
            nltk.download('words', quiet=True)
    
    def analyze_job_posting(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì±„ìš©ê³µê³  ì¢…í•© ë¶„ì„
        
        Args:
            job_data: ì±„ìš©ê³µê³  ë°ì´í„°
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = self._extract_text_content(job_data)
            
            # ê¸°ë³¸ ë¶„ì„
            basic_analysis = self._basic_text_analysis(text_content)
            
            # í‚¤ì›Œë“œ ë¶„ì„
            keyword_analysis = self._extract_keywords(text_content)
            
            # ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„
            tech_analysis = self._analyze_tech_stack(text_content)
            
            # ìš”êµ¬ì‚¬í•­ ë¶„ì„
            requirements_analysis = self._analyze_requirements(text_content)
            
            # ì§ë¬´ ë¶„ë¥˜
            job_category = self._classify_job_category(text_content)
            
            # ê²½ë ¥ ìš”êµ¬ì‚¬í•­ ë¶„ì„
            experience_analysis = self._analyze_experience_requirements(text_content)
            
            # íšŒì‚¬ ì •ë³´ ë¶„ì„
            company_analysis = self._analyze_company_info(job_data)
            
            # ê¸‰ì—¬ ì •ë³´ ë¶„ì„
            salary_analysis = self._analyze_salary_info(text_content)
            
            return {
                'basic_info': {
                    'company_name': job_data.get('company_name', ''),
                    'position': job_data.get('position', ''),
                    'job_category': job_category,
                    'analysis_date': self._get_current_datetime()
                },
                'text_analysis': basic_analysis,
                'keywords': keyword_analysis,
                'tech_stack': tech_analysis,
                'requirements': requirements_analysis,
                'experience': experience_analysis,
                'company_info': company_analysis,
                'salary_info': salary_analysis,
                'score': self._calculate_overall_score(keyword_analysis, tech_analysis, requirements_analysis)
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'analysis_date': self._get_current_datetime()
            }
    
    def _extract_text_content(self, job_data: Dict[str, Any]) -> str:
        """ì±„ìš©ê³µê³ ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
        content_parts = []
        
        # ì œëª©
        if job_data.get('title'):
            content_parts.append(job_data['title'])
        
        # íšŒì‚¬ëª…
        if job_data.get('company_name'):
            content_parts.append(job_data['company_name'])
        
        # ë³¸ë¬¸ ë‚´ìš©
        if job_data.get('content'):
            content_parts.append(job_data['content'])
        
        # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if job_data.get('html_content'):
            soup = BeautifulSoup(job_data['html_content'], 'html.parser')
            content_parts.append(soup.get_text())
        
        return ' '.join(content_parts)
    
    def _basic_text_analysis(self, text: str) -> Dict[str, Any]:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„"""
        # ë¬¸ì¥ ë° ë‹¨ì–´ ìˆ˜
        sentences = sent_tokenize(text)
        words = word_tokenize(text.lower())
        
        # í•œê¸€ ë‹¨ì–´ë§Œ í•„í„°ë§
        korean_words = [word for word in words if re.match(r'^[ê°€-í£]+$', word)]
        
        return {
            'total_characters': len(text),
            'total_sentences': len(sentences),
            'total_words': len(words),
            'korean_words': len(korean_words),
            'avg_sentence_length': len(words) / len(sentences) if sentences else 0,
            'reading_time_minutes': len(words) / 200  # í‰ê·  ì½ê¸° ì†ë„ 200ë‹¨ì–´/ë¶„
        }
    
    def _extract_keywords(self, text: str) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„"""
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        filtered_words = [word for word in korean_words if word not in self.korean_stopwords]
        
        # ë¹ˆë„ ë¶„ì„
        word_freq = Counter(filtered_words)
        
        # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        english_words = re.findall(r'[A-Za-z]{3,}', text)
        english_freq = Counter([word.lower() for word in english_words])
        
        return {
            'top_korean_keywords': word_freq.most_common(20),
            'top_english_keywords': english_freq.most_common(15),
            'total_unique_words': len(word_freq) + len(english_freq),
            'keyword_density': len(filtered_words) / len(text.split()) if text.split() else 0
        }
    
    def _analyze_tech_stack(self, text: str) -> Dict[str, Any]:
        """ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„"""
        found_tech = defaultdict(list)
        
        for category, keywords in self.tech_keywords.items():
            for keyword in keywords:
                # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰
                if re.search(rf'\b{re.escape(keyword)}\b', text, re.IGNORECASE):
                    found_tech[category].append(keyword)
        
        # ê¸°ìˆ  ìŠ¤íƒ ì ìˆ˜ ê³„ì‚°
        tech_score = sum(len(techs) for techs in found_tech.values())
        
        return {
            'found_technologies': dict(found_tech),
            'tech_diversity_score': len(found_tech),
            'total_tech_count': tech_score,
            'most_required_category': max(found_tech.items(), key=lambda x: len(x[1]))[0] if found_tech else None
        }
    
    def _analyze_requirements(self, text: str) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        # í•„ìˆ˜/ìš°ëŒ€ ì¡°ê±´ íŒ¨í„´
        required_patterns = [
            r'í•„ìˆ˜.*?(?=ìš°ëŒ€|ìê²©|í˜œíƒ|ê·¼ë¬´|$)',
            r'ìš”êµ¬ì‚¬í•­.*?(?=ìš°ëŒ€|ìê²©|í˜œíƒ|ê·¼ë¬´|$)',
            r'ì§€ì›ìê²©.*?(?=ìš°ëŒ€|ìê²©|í˜œíƒ|ê·¼ë¬´|$)'
        ]
        
        preferred_patterns = [
            r'ìš°ëŒ€.*?(?=ìê²©|í˜œíƒ|ê·¼ë¬´|ë³µë¦¬|$)',
            r'ê°€ì‚°ì .*?(?=ìê²©|í˜œíƒ|ê·¼ë¬´|ë³µë¦¬|$)'
        ]
        
        required_items = []
        preferred_items = []
        
        # í•„ìˆ˜ ì¡°ê±´ ì¶”ì¶œ
        for pattern in required_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                items = re.split(r'[â€¢\-\*\n]', match.strip())
                required_items.extend([item.strip() for item in items if item.strip()])
        
        # ìš°ëŒ€ ì¡°ê±´ ì¶”ì¶œ
        for pattern in preferred_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                items = re.split(r'[â€¢\-\*\n]', match.strip())
                preferred_items.extend([item.strip() for item in items if item.strip()])
        
        return {
            'required_qualifications': required_items[:10],  # ìƒìœ„ 10ê°œë§Œ
            'preferred_qualifications': preferred_items[:10],
            'total_requirements': len(required_items) + len(preferred_items)
        }
    
    def _classify_job_category(self, text: str) -> str:
        """ì§ë¬´ ë¶„ë¥˜"""
        category_scores = {}
        
        for category, keywords in self.job_categories.items():
            score = 0
            for keyword in keywords:
                score += len(re.findall(rf'\b{re.escape(keyword)}\b', text, re.IGNORECASE))
            category_scores[category] = score
        
        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        return 'general'
    
    def _analyze_experience_requirements(self, text: str) -> Dict[str, Any]:
        """ê²½ë ¥ ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        # ê²½ë ¥ ê´€ë ¨ íŒ¨í„´
        experience_patterns = [
            r'(\d+)ë…„\s*ì´ìƒ',
            r'ê²½ë ¥\s*(\d+)ë…„',
            r'(\d+)ë…„ì°¨',
            r'ì‹ ì…|ì‹ ê·œ|new|junior',
            r'ì‹œë‹ˆì–´|senior|ì±…ì„|ì„ ì„'
        ]
        
        experience_info = {
            'min_years': None,
            'max_years': None,
            'is_entry_level': False,
            'is_senior_level': False,
            'experience_keywords': []
        }
        
        # ë…„ìˆ˜ ì¶”ì¶œ
        years = []
        for pattern in experience_patterns[:3]:
            matches = re.findall(pattern, text, re.IGNORECASE)
            years.extend([int(match) for match in matches if match.isdigit()])
        
        if years:
            experience_info['min_years'] = min(years)
            experience_info['max_years'] = max(years)
        
        # ì‹ ì…/ì‹œë‹ˆì–´ ì—¬ë¶€
        if re.search(r'ì‹ ì…|ì‹ ê·œ|new|junior', text, re.IGNORECASE):
            experience_info['is_entry_level'] = True
        
        if re.search(r'ì‹œë‹ˆì–´|senior|ì±…ì„|ì„ ì„', text, re.IGNORECASE):
            experience_info['is_senior_level'] = True
        
        return experience_info
    
    def _analyze_company_info(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """íšŒì‚¬ ì •ë³´ ë¶„ì„"""
        company_info = {
            'company_name': job_data.get('company_name', ''),
            'industry': job_data.get('industry', ''),
            'company_size': job_data.get('company_size', ''),
            'location': job_data.get('location', ''),
            'company_type': self._classify_company_type(job_data.get('company_name', ''))
        }
        
        return company_info
    
    def _classify_company_type(self, company_name: str) -> str:
        """íšŒì‚¬ ìœ í˜• ë¶„ë¥˜"""
        if not company_name:
            return 'unknown'
        
        # ëŒ€ê¸°ì—… íŒ¨í„´
        large_company_patterns = ['ì‚¼ì„±', 'LG', 'SK', 'í˜„ëŒ€', 'ë¡¯ë°', 'KT', 'CJ', 'í•œí™”', 'í¬ìŠ¤ì½”', 'GS']
        if any(pattern in company_name for pattern in large_company_patterns):
            return 'large_enterprise'
        
        # ìŠ¤íƒ€íŠ¸ì—… íŒ¨í„´
        startup_patterns = ['ìŠ¤íƒ€íŠ¸ì—…', 'startup', 'ë²¤ì²˜']
        if any(pattern in company_name.lower() for pattern in startup_patterns):
            return 'startup'
        
        # IT íšŒì‚¬ íŒ¨í„´
        it_patterns = ['í…Œí¬', 'tech', 'IT', 'ì†Œí”„íŠ¸ì›¨ì–´', 'software']
        if any(pattern in company_name.lower() for pattern in it_patterns):
            return 'tech_company'
        
        return 'general'
    
    def _analyze_salary_info(self, text: str) -> Dict[str, Any]:
        """ê¸‰ì—¬ ì •ë³´ ë¶„ì„"""
        salary_info = {
            'has_salary_info': False,
            'salary_range': None,
            'salary_type': None,  # annual, monthly, hourly
            'benefits': []
        }
        
        # ê¸‰ì—¬ íŒ¨í„´
        salary_patterns = [
            r'ì—°ë´‰\s*(\d+(?:,\d+)*)\s*ë§Œì›',
            r'ì›”ê¸‰\s*(\d+(?:,\d+)*)\s*ë§Œì›',
            r'ì‹œê¸‰\s*(\d+(?:,\d+)*)\s*ì›'
        ]
        
        for i, pattern in enumerate(salary_patterns):
            matches = re.findall(pattern, text)
            if matches:
                salary_info['has_salary_info'] = True
                salary_info['salary_range'] = matches[0]
                salary_info['salary_type'] = ['annual', 'monthly', 'hourly'][i]
                break
        
        # ë³µë¦¬í›„ìƒ í‚¤ì›Œë“œ
        benefit_keywords = ['4ëŒ€ë³´í—˜', 'í‡´ì§ê¸ˆ', 'ì—°ì°¨', 'íœ´ê°€', 'êµìœ¡ì§€ì›', 'ì‹ëŒ€', 'êµí†µë¹„', 'ê±´ê°•ê²€ì§„']
        found_benefits = [keyword for keyword in benefit_keywords if keyword in text]
        salary_info['benefits'] = found_benefits
        
        return salary_info
    
    def _calculate_overall_score(self, keywords: Dict, tech: Dict, requirements: Dict) -> float:
        """ì „ì²´ì ì¸ ì±„ìš©ê³µê³  ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ì ìˆ˜ (0-30)
        keyword_score = min(len(keywords.get('top_korean_keywords', [])) * 1.5, 30)
        
        # ê¸°ìˆ  ìŠ¤íƒ ì ìˆ˜ (0-40)
        tech_score = min(tech.get('total_tech_count', 0) * 4, 40)
        
        # ìš”êµ¬ì‚¬í•­ ëª…í™•ì„± ì ìˆ˜ (0-30)
        req_score = min(requirements.get('total_requirements', 0) * 3, 30)
        
        total_score = keyword_score + tech_score + req_score
        return round(total_score, 1)
    
    def _get_current_datetime(self) -> str:
        """í˜„ì¬ ë‚ ì§œì‹œê°„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def generate_analysis_report(self, analysis_result: Dict[str, Any]) -> str:
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if 'error' in analysis_result:
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {analysis_result['error']}"
        
        report = []
        
        # ê¸°ë³¸ ì •ë³´
        basic = analysis_result.get('basic_info', {})
        report.append(f"# ğŸ“Š ì±„ìš©ê³µê³  ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append(f"**íšŒì‚¬ëª…**: {basic.get('company_name', 'N/A')}")
        report.append(f"**ì§ë¬´**: {basic.get('position', 'N/A')}")
        report.append(f"**ë¶„ë¥˜**: {basic.get('job_category', 'N/A')}")
        report.append(f"**ë¶„ì„ì¼ì‹œ**: {basic.get('analysis_date', 'N/A')}")
        report.append("")
        
        # ì „ì²´ ì ìˆ˜
        score = analysis_result.get('score', 0)
        report.append(f"## ğŸ¯ ì¢…í•© ì ìˆ˜: {score}/100")
        report.append("")
        
        # ê¸°ìˆ  ìŠ¤íƒ
        tech = analysis_result.get('tech_stack', {})
        if tech.get('found_technologies'):
            report.append("## ğŸ› ï¸ ìš”êµ¬ ê¸°ìˆ  ìŠ¤íƒ")
            for category, techs in tech['found_technologies'].items():
                if techs:
                    report.append(f"**{category.title()}**: {', '.join(techs)}")
            report.append("")
        
        # í‚¤ì›Œë“œ ë¶„ì„
        keywords = analysis_result.get('keywords', {})
        if keywords.get('top_korean_keywords'):
            report.append("## ğŸ” ì£¼ìš” í‚¤ì›Œë“œ")
            top_keywords = keywords['top_korean_keywords'][:10]
            keyword_list = [f"{word} ({count})" for word, count in top_keywords]
            report.append(", ".join(keyword_list))
            report.append("")
        
        # ê²½ë ¥ ìš”êµ¬ì‚¬í•­
        exp = analysis_result.get('experience', {})
        report.append("## ğŸ‘¨â€ğŸ’¼ ê²½ë ¥ ìš”êµ¬ì‚¬í•­")
        if exp.get('min_years'):
            report.append(f"**ìµœì†Œ ê²½ë ¥**: {exp['min_years']}ë…„")
        if exp.get('is_entry_level'):
            report.append("**ì‹ ì… ê°€ëŠ¥**: âœ…")
        if exp.get('is_senior_level'):
            report.append("**ì‹œë‹ˆì–´ ë ˆë²¨**: âœ…")
        report.append("")
        
        # ê¸‰ì—¬ ì •ë³´
        salary = analysis_result.get('salary_info', {})
        if salary.get('has_salary_info'):
            report.append("## ğŸ’° ê¸‰ì—¬ ì •ë³´")
            report.append(f"**ê¸‰ì—¬ ë²”ìœ„**: {salary.get('salary_range', 'N/A')}")
            report.append(f"**ê¸‰ì—¬ ìœ í˜•**: {salary.get('salary_type', 'N/A')}")
            if salary.get('benefits'):
                report.append(f"**ë³µë¦¬í›„ìƒ**: {', '.join(salary['benefits'])}")
            report.append("")
        
        return "\n".join(report)

def main():
    """ë©”ì¸ í•¨ìˆ˜ - í…ŒìŠ¤íŠ¸ìš©"""
    analyzer = JobPostingAnalyzer()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_job_data = {
        'company_name': 'ì‚¼ì„±ì „ì',
        'position': 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì',
        'title': 'ì‚¼ì„±ì „ì ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì ì±„ìš©',
        'content': '''
        ì‚¼ì„±ì „ìì—ì„œ ìš°ìˆ˜í•œ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìë¥¼ ëª¨ì§‘í•©ë‹ˆë‹¤.
        
        [ì§€ì›ìê²©]
        - ì»´í“¨í„°ê³µí•™ ê´€ë ¨ í•™ê³¼ ì¡¸ì—…ì
        - Java, Python, JavaScript ê°œë°œ ê²½í—˜ 3ë…„ ì´ìƒ
        - Spring Framework, React ì‚¬ìš© ê²½í—˜
        - AWS í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ê²½í—˜
        
        [ìš°ëŒ€ì‚¬í•­]
        - Docker, Kubernetes ê²½í—˜ì
        - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê²½í—˜
        - ì˜ì–´ íšŒí™” ê°€ëŠ¥ì
        
        [ê·¼ë¬´ì¡°ê±´]
        - ì—°ë´‰ 5000ë§Œì› ì´ìƒ
        - 4ëŒ€ë³´í—˜, í‡´ì§ê¸ˆ, ì—°ì°¨ ì œê³µ
        - êµìœ¡ë¹„ ì§€ì›
        '''
    }
    
    # ë¶„ì„ ì‹¤í–‰
    result = analyzer.analyze_job_posting(test_job_data)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = analyzer.generate_analysis_report(result)
    
    print("=== ì±„ìš©ê³µê³  ë¶„ì„ ê²°ê³¼ ===")
    print(report)
    print("\n=== JSON ê²°ê³¼ ===")
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
